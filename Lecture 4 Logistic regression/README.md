# Lecture 4
## å¿«é€Ÿé€£çµ
ä»¥ä¸‹é€£çµéœ€è¦ç€è¦½å™¨ç™»å…¥ NCHU å­¸ç”Ÿå¸³è™Ÿï¼Œæ‰æœƒæ­£å¸¸é‹ä½œã€‚æˆ–æ˜¯å¯åœ¨åŒå€‹è³‡æ–™å¤¾å°‹æ‰¾åŒåæª”æ¡ˆã€‚
- [ç·šä¸Š Google Meeting é€£çµ](https://lms2020.nchu.edu.tw/media/doc/86493)

é»é¸ä¸‹æ–¹ ```Open with Colab``` å³å¯ç›´æ¥é–‹å•Ÿæª”æ¡ˆè‡³ Colabï¼Œç•™æ„ ```data.csv``` ä¸æœƒè¢«è¤‡è£½éå»ã€‚

[Lecture 4 Logistic_Regression]()

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/awinlab/CourseTemplateTry/blob/main/Lecture%204%20Logistic%20regression/Multi-Linear%20regression%202021.10.2(Answer%20sample%20NotUseSelection)%20.ipynb)
## Review ä¸Šæ¬¡å…§å®¹
* ç·šæ€§å›æ­¸,æˆ‘å€‘ä»‹ç´¹ä»€éº¼æ˜¯Excelæ“ä½œä¸¦ç”¨å¹¾å€‹ä¾‹å­èªªæ˜ç·šæ€§å›æ­¸çš„æ‡‰ç”¨, æ„Ÿæ¸¬å™¨è¼ƒæ­£
* åœ¨ç”¨Python ä¾†å¯¦ç¾=>google  Coble
* ML å¯¦ä½œæ–¹æ³•ç†Ÿæ‚‰ä¸€ä¸‹follow CRISP-DM (è·¨é ˜åŸŸè³‡æ–™åˆ†æä½œæ¥­æµç¨‹) æ–¹æ³•è«–ä¾†å»ºç«‹ML æ¨¡å‹
    * step1:ç²å–è³‡æ–™
        * Kaggle 2000+ opendata CSV
        * çˆ¬èŸ² FindMind XML,GET/POST,å®šæ™‚çˆ¬èŸ²
        * API (æ°¸è±,Google,å¤©æ°£) REST
    * step2:æ¸…ç†Data
        * UNet
        * LabelImg
        * LabelMe
    * step3:select model
    * step4:evaluate model<br>
        ![evaluate_model_1](./images/evaluate_model_1.png)
        ![evaluate_model_2](./images/evaluate_model_2.png)
    * step5:deploy model
## å¤šå…ƒç·šæ€§å›æ­¸
### å•é¡Œç‰¹è‰²
1. Linearity (å› è®Šæ•¸xi è·Ÿyä¹‹é–“æœ‰ç·šæ€§é—œä¿‚)
2. Homoscendasticity (æ¯ä¸€å€‹xi æ–¹å·®éƒ½ä¸€æ¨£)
3. Mulivariate normality (æ¯ä¸€å€‹xi normal distribution)
4. Independence of error (èª¤å·®ç¨ç«‹)
5. Lack of multicollinearity (ä¸å¯æœ‰å…±ç·šæ€§)

### å¯¦ä½œä¸Šæ²’è¾¦æ³•ä¸€ä¸€æª¢é©—, æ‰€ä»¥ç”¨ä¸‹é¢é€™ç¨®å·¥ç¨‹æ–¹æ³•ä¾†æŒ‘é¸é©åˆçš„ç‰¹å¾µçµ„
1. All-in (ä½•æ™‚å¯ç”¨ğŸ¡º ä½ æ˜¯å°ˆå®¶, æ»¿è¶³ä¸Šé¢äº”å€‹è¦æ±‚, Con: ç•¶ç‰¹å¾µå¾ˆå¤šæ™‚ 100å€‹ä»¥ä¸Š, ä½ å°±ç®—æ˜¯å°ˆå®¶, ä½ ä¹Ÿå¾ˆé›£å®Œå…¨ä¿è­‰)
2. Stepwise Selection (greedy method)
    1. Backward selection (æœ€å¸¸ç”¨)<br>
        ç¯©é¸æ¨™æº– p-value (p-value < 0.05 é¡¯è‘—, <0.01éå¸¸é¡¯è‘—) å‡ºéŒ¯çš„æ©Ÿç‡<br>
        ![backward_selection](./images/backward_selection.png)
    2. Forward selection<br>
        ![forward_selection](./images/forward_selection.png)
    3. Bidirection selection
3. Score selection (feature selection optimization)

å‡è¨­æœ‰ 10 ç‰¹å¾µå¯é¸, å…±æœ‰2^10=1024 ç‰¹å¾µé›†çš„é¸æ“‡<br>
æš´åŠ›ç ´è§£å¯æ‰¾åˆ°æœ€ä½³ (sklearn Grid Serarch)<br>
å¯æ˜¯å£è™•å°±æ˜¯æ²’æ™‚é–“å› æ­¤è¦è¨´è«¸æ‰€è¬‚çš„æœå°‹æ¼”ç®—æ³•<br>
é€²éšçš„æœå°‹æ¼”ç®—æ³•ä½¿ç”¨Meta-heuristic<br>
åƒè€ƒ è¯åˆå¤§å­¸é™³å£«æ°
[èª²ç¨‹](http://debussy.im.nuu.edu.tw/sjchen/ML_final.html)<br>
ä¸»è¦æœ‰å: Hill climbing algorithm, GA, PSO, SA, TSâ€¦

## DATA å‹æ…‹
Nominal data éœ€è¦åšOneHotEncoding (1 to n ) ç”¢ç”Ÿæ•¸å€‹DUMMY VARIABLE (ç‰¹å¾µ)<br>
é€™æ™‚åˆç”¢ç”Ÿå¦ä¸€å€‹å•é¡Œ Dummy variable trap é™·é˜±(å¦‚æœnå€‹éƒ½ç”¨çš„è©±ä¸€å®šæœ€å¾Œä¸€å€‹æ˜¯ç·šæ€§ç›¸ä¾ xn=1-x1-x2-â€¦-xn-1)<br>
SolutionğŸ¡ºæ–°ç”¢ç”Ÿn å€‹åªèƒ½æ”¾n-1é€²å»å¼å­è£¡é¢

Label Encoding å‹•ä½œ (1 to 1)== Label

çœ‹p-value  [sample code](https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression)
